{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJ7kXqqR4+HjEFMBbJ/AT8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdithiyaG/Langchain/blob/main/Langchain/DLAI/Functions%2CTools%2CAgents_with_Langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions,Tools,Agents with Langchain by Harison Chase\n",
        "Deeplearing.ai short course\n",
        "\n",
        "\n",
        "> 30 April 2024\n",
        "\n"
      ],
      "metadata": {
        "id": "fKolGbMELPne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "oJpKLFfFOxQc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53azmF-e02Ik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2da7bfa1-ce63-490c-b0cf-b4107198c261"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.9/312.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import AzureOpenAI"
      ],
      "metadata": {
        "id": "uRXLkmBs2E5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"]=userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"AZURE_OPENAI_ENDPOINT\"]=userdata.get('OPENAI_ENDPOINT')\n",
        "os.environ[\"OPENAI_API_VERSION\"]=\"2024-02-15-preview\"\n",
        "\n",
        "client = AzureOpenAI()"
      ],
      "metadata": {
        "id": "8_TARdgC1H3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiulSo1sG9GY",
        "outputId": "2f22a0a0-8263-46c2-a374-510ce832238b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m867.6/867.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.4/116.4 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.7/142.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai  import AzureChatOpenAI\n",
        "model=AzureChatOpenAI(model=\"gpt-4-turbo\")\n",
        "model_gpt_4=AzureChatOpenAI(model=\"gpt-4-32k\")"
      ],
      "metadata": {
        "id": "3GxAul-sQGja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lesson 1"
      ],
      "metadata": {
        "id": "G3p1r0mAKuL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def get_current_weather(location, unit=\"fahrenheit\"):\n",
        "    \"\"\"Get the current weather in a given location\"\"\"\n",
        "    weather_info = {\n",
        "        \"location\": location,\n",
        "        \"temperature\": \"72\",\n",
        "        \"unit\": unit,\n",
        "        \"forecast\": [\"sunny\", \"windy\"],\n",
        "    }\n",
        "    return json.dumps(weather_info)"
      ],
      "metadata": {
        "id": "D_dhrKIV2VrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_current_weather(\"SF\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_FEh43yl22wQ",
        "outputId": "661a7d92-4551-40f3-cd2c-4f434dbb643c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\"location\": \"SF\", \"temperature\": \"72\", \"unit\": \"fahrenheit\", \"forecast\": [\"sunny\", \"windy\"]}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenAI function format\n"
      ],
      "metadata": {
        "id": "90X8pBEU3dZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "functions = [\n",
        "    {\n",
        "        \"name\": \"get_current_weather\",\n",
        "        \"description\": \"Get the current weather in a given location\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"location\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
        "                },\n",
        "                \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
        "            },\n",
        "            \"required\": [\"location\"],\n",
        "        },\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "NXNQulnK6wlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"What's the weather like in Boston?\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "CA43GIa54qQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response=client.chat.completions.create(\n",
        "    model=\"gpt-4-32k\",\n",
        "    messages=messages,\n",
        "    functions=functions,\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "HvtHq_5q4wQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5PrtVOW7X-c",
        "outputId": "d7e9acc2-f901-4b60-ad97-91afcc131fba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-9JcINfUAabfdZMKReFELhQ1KaWb3m', choices=[Choice(finish_reason='function_call', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=FunctionCall(arguments='{\\n  \"location\": \"Boston, MA\"\\n}', name='get_current_weather'), tool_calls=None), content_filter_results={})], created=1714462467, model='gpt-4-32k', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=18, prompt_tokens=82, total_tokens=100), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4ckiuTiNEPuH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Lesson 3\n"
      ],
      "metadata": {
        "id": "Met6DpADEPZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pydantic\n",
        "Pydantic is data validation library for python\n",
        "\n",
        "* works with python type annotations.but rather than static type checking,they are actively used at runtime for dta validation\n",
        "\n",
        "* Provides built in methods sserialize/deserialize methods to/from JSON,dict etc\n",
        "\n",
        "* Langchain leverages Pydantic to create JSON scheme describing function"
      ],
      "metadata": {
        "id": "jCGRsr5PEYak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel,Field"
      ],
      "metadata": {
        "id": "mOc5a295FP9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class User:\n",
        "  def __init__(self,name:str,age:int,email:str):\n",
        "    self.name=name\n",
        "    self.age=age\n",
        "    self.email=email"
      ],
      "metadata": {
        "id": "qrsUu1RLFEih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "foo=User(\"Joe\",32,\"abc@x.com\")"
      ],
      "metadata": {
        "id": "Ba-8YHzyFnJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "foo.age"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qdq-fo8Fvnk",
        "outputId": "2177c25c-a0e9-45cc-883a-dc043569dfc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "foo=User(\"Joe\",\"NA\",\"abc@x.com\")"
      ],
      "metadata": {
        "id": "1K-geHaQFyhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "foo.age"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tpnRNA-PF3KB",
        "outputId": "56ce189e-90d4-4e76-d9d3-ca0cb3fc45fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'NA'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class pUser(BaseModel):\n",
        "  name:str\n",
        "  age:int\n",
        "  email:str"
      ],
      "metadata": {
        "id": "x8A6IxbAF5mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "foo=pUser(name=\"Joe\",age=\"NA\",email=\"abc@x.com\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "i6htiIMyGDzk",
        "outputId": "097885fd-e452-44e1-dc04-3538e87164f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "1 validation error for pUser\nage\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='NA', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.7/v/int_parsing",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-fdb03af499f9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfoo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpUser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Joe\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"NA\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0memail\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"abc@x.com\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;31m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pydantic_validator__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;31m# The following line sets a flag that we use to determine when `__init__` gets overridden by the user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for pUser\nage\n  Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='NA', input_type=str]\n    For further information visit https://errors.pydantic.dev/2.7/v/int_parsing"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pydantic to OpenAI functions"
      ],
      "metadata": {
        "id": "lHju2T5eGqU8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wcsd4iivOf4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function description is necessary, Field Description is optional"
      ],
      "metadata": {
        "id": "WE0yoxG3IYvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WeatherSearch(BaseModel):\n",
        "    \"\"\"Call this with an airport code to get the weather at that airport\"\"\"\n",
        "    airport_code: str = Field(description=\"airport code to get weather for\")"
      ],
      "metadata": {
        "id": "MU59g7iRGmae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.utils.function_calling import convert_to_openai_function"
      ],
      "metadata": {
        "id": "iFRFbvA6G6Nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "convert_to_openai_function fails to capture Field description cmp to convert_pydantic_to_openai_functions(deprecrated)"
      ],
      "metadata": {
        "id": "LqzBvoI8ORWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weather_function"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkkQ8l9qHqKs",
        "outputId": "a140d7e9-e31e-4b84-9ac5-86b590274cff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'WeatherSearch',\n",
              " 'description': 'Call this with an airport code to get the weather at that airport',\n",
              " 'parameters': {'type': 'object',\n",
              "  'properties': {'airport_code': {'type': 'string'}},\n",
              "  'required': ['airport_code']}}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai  import AzureChatOpenAI"
      ],
      "metadata": {
        "id": "XzFPc85FH3oW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=AzureChatOpenAI(model=\"gpt-4-32k\")"
      ],
      "metadata": {
        "id": "vGfESU8uImk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.invoke(\"what is the weather in SF today?\", functions=[weather_function])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdwS-B3hI28d",
        "outputId": "a76ec10a-18d0-4cc8-d8cb-bb286d3df92d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"airport_code\": \"SFO\"\\n}', 'name': 'WeatherSearch'}}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 62, 'total_tokens': 80}, 'model_name': 'gpt-4-32k', 'system_fingerprint': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'function_call', 'logprobs': None, 'content_filter_results': {}}, id='run-1617ec15-3c19-4c02-a284-02bc15029fd9-0')"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_functions=model.bind(functions=[weather_function])\n",
        "model_with_functions.invoke(\"what is the weather in SF today?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8I07HiqJdPK",
        "outputId": "ec89e511-4603-4b87-a3e7-922dc97b78e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"airport_code\": \"SFO\"\\n}', 'name': 'WeatherSearch'}}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 62, 'total_tokens': 80}, 'model_name': 'gpt-4-32k', 'system_fingerprint': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'function_call', 'logprobs': None, 'content_filter_results': {}}, id='run-ed27a676-46f7-42a3-aa92-188d07cd4660-0')"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_functions.invoke(\"hi!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cZ3TbjDJ7Hz",
        "outputId": "3691e828-7e87-4211-fe3c-44b99215a153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 56, 'total_tokens': 66}, 'model_name': 'gpt-4-32k', 'system_fingerprint': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-1de05419-3020-4fb3-83cc-54e941ef18fa-0')"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forcing Function Call"
      ],
      "metadata": {
        "id": "xe7ns0AVKL7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_forced_function = model.bind(functions=[weather_function], function_call={\"name\":\"WeatherSearch\"})"
      ],
      "metadata": {
        "id": "-krqBzSaKDHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_forced_function.invoke(\"what is the weather in sf?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6v0-duyKIaj",
        "outputId": "965d71b6-3ec6-4e94-8be6-15f09923f34f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"airport_code\": \"SFO\"\\n}', 'name': 'WeatherSearch'}}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 68, 'total_tokens': 79}, 'model_name': 'gpt-4-32k', 'system_fingerprint': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {}}, id='run-18f314be-470f-44e4-8a7d-0b7c2360ff41-0')"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_forced_function.invoke(\"hi!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhJYE4kuKK9e",
        "outputId": "8b400166-6d9b-45b4-b61b-c5aa37aeb980"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"airport_code\": \"JFK\"\\n}', 'name': 'WeatherSearch'}}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 63, 'total_tokens': 74}, 'model_name': 'gpt-4-32k', 'system_fingerprint': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {}}, id='run-71d5a831-26e7-4063-80fb-278e26aa5ce1-0')"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lesson 4 Tagging and Extraction"
      ],
      "metadata": {
        "id": "vUSisYZgK6S7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tagging"
      ],
      "metadata": {
        "id": "S9WHB9neQuJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tagging and Extraction Using OpenAI functions\n",
        "\n",
        "from typing import List\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain.utils.openai_functions import convert_pydantic_to_openai_function\n",
        "\n",
        "class Tagging(BaseModel):\n",
        "    \"\"\"Tag the piece of text with particular info.\"\"\"\n",
        "    sentiment: str = Field(description=\"sentiment of text, should be `pos`, `neg`, or `neutral`\")\n",
        "    language: str = Field(description=\"language of text (should be ISO 639-1 code)\")\n"
      ],
      "metadata": {
        "id": "e-PTUaMgO9Vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convert_pydantic_to_openai_function(Tagging)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2hcEPBKPBdp",
        "outputId": "8bcdfef4-7823-4234-dcb7-010acdc8d93a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The function `convert_pydantic_to_openai_function` was deprecated in LangChain 0.1.16 and will be removed in 0.2.0. Use langchain_core.utils.function_calling.convert_to_openai_function() instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'Tagging',\n",
              " 'description': 'Tag the piece of text with particular info.',\n",
              " 'parameters': {'properties': {'sentiment': {'description': 'sentiment of text, should be `pos`, `neg`, or `neutral`',\n",
              "    'type': 'string'},\n",
              "   'language': {'description': 'language of text (should be ISO 639-1 code)',\n",
              "    'type': 'string'}},\n",
              "  'required': ['sentiment', 'language'],\n",
              "  'type': 'object'}}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "\n",
        "\n",
        "tagging_functions = [convert_pydantic_to_openai_function(Tagging)]\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Think carefully, and then tag the text as instructed\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "\n",
        "model_with_functions = model.bind(\n",
        "    functions=tagging_functions,\n",
        "    function_call={\"name\": \"Tagging\"}\n",
        ")\n",
        "\n",
        "tagging_chain = prompt | model_with_functions\n",
        "\n"
      ],
      "metadata": {
        "id": "bCjaXa84Pt8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagging_chain.invoke({\"input\": \"I love langchain\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ila56QtSQTP3",
        "outputId": "a2fb6cfe-6376-4e68-d7f6-03a1c597c2ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n\"sentiment\": \"pos\",\\n\"language\": \"en\"\\n}', 'name': 'Tagging'}}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 105, 'total_tokens': 120}, 'model_name': 'gpt-4-32k', 'system_fingerprint': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {}}, id='run-4c2d712c-75af-4346-aa25-b7af2bef1511-0')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tagging_chain.invoke({\"input\": \"non mi piace questo cibo\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gduYrzCaQXyu",
        "outputId": "a3ad517c-7e33-4e29-85c2-71ba9b826743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n\"sentiment\": \"neg\",\\n\"language\": \"it\"\\n}', 'name': 'Tagging'}}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 108, 'total_tokens': 123}, 'model_name': 'gpt-4-32k', 'system_fingerprint': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {}}, id='run-aec59328-db3c-4b43-9c82-189dae790bb4-0')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "JsonOutputFunctionsParser"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "w1c2e-u0Qkz8",
        "outputId": "85522fec-b0f0-4b82-9bfb-8cf67dbe53f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain_core.output_parsers.openai_functions.JsonOutputFunctionsParser"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.output_parsers.openai_functions.JsonOutputFunctionsParser</b><br/>def __init__(**kwargs: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/langchain_core/output_parsers/openai_functions.py</a>Parse an output as the Json object.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 40);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
        "\n",
        "tagging_chain = prompt | model_with_functions | JsonOutputFunctionsParser()\n",
        "\n",
        "tagging_chain.invoke({\"input\": \"non mi piace questo cibo\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_AXchsNQQtg",
        "outputId": "31f80697-ce8f-481c-b9ea-ea30de82a42c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentiment': 'neg', 'language': 'it'}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extraction\n",
        "Extraction is similar to tagging, but used for extracting multiple pieces of information."
      ],
      "metadata": {
        "id": "lsH2BzPoQ0Ix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "class Person(BaseModel):\n",
        "    \"\"\"Information about a person.\"\"\"\n",
        "    name: str = Field(description=\"person's name\")\n",
        "    age: Optional[int] = Field(description=\"person's age\")\n",
        "\n",
        "class Information(BaseModel):\n",
        "    \"\"\"Information to extract.\"\"\"\n",
        "    people: List[Person] = Field(description=\"List of info about people\")"
      ],
      "metadata": {
        "id": "0RBjvS_6RN12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convert_pydantic_to_openai_function(Information)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yURw4WbGRQCM",
        "outputId": "f39de6ac-9e44-4202-a1cc-79a0b75f6656"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'Information',\n",
              " 'description': 'Information to extract.',\n",
              " 'parameters': {'$defs': {'Person': {'description': 'Information about a person.',\n",
              "    'properties': {'name': {'description': \"person's name\", 'type': 'string'},\n",
              "     'age': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
              "      'description': \"person's age\"}},\n",
              "    'required': ['name', 'age'],\n",
              "    'type': 'object'}},\n",
              "  'properties': {'people': {'description': 'List of info about people',\n",
              "    'items': {'description': 'Information about a person.',\n",
              "     'properties': {'name': {'description': \"person's name\", 'type': 'string'},\n",
              "      'age': {'anyOf': [{'type': 'integer'}, {'type': 'null'}],\n",
              "       'description': \"person's age\"}},\n",
              "     'required': ['name', 'age'],\n",
              "     'type': 'object'},\n",
              "    'type': 'array'}},\n",
              "  'required': ['people'],\n",
              "  'type': 'object'}}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extraction_functions = [convert_pydantic_to_openai_function(Information)]\n",
        "extraction_model = model.bind(functions=extraction_functions, function_call={\"name\": \"Information\"})"
      ],
      "metadata": {
        "id": "jBz5m_TOR_dF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extraction_model.invoke(\"Joe is 30, his mom is Martha\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDma2V-fSDym",
        "outputId": "b78951d9-1c56-4850-dd11-d96b55064dd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"people\": [\\n    {\\n      \"name\": \"Joe\",\\n      \"age\": 30\\n    },\\n    {\\n      \"name\": \"Martha\",\\n      \"age\": null\\n    }\\n  ]\\n}', 'name': 'Information'}}, response_metadata={'token_usage': {'completion_tokens': 45, 'prompt_tokens': 76, 'total_tokens': 121}, 'model_name': 'gpt-4-32k', 'system_fingerprint': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {}}, id='run-6f3de8ec-e0f4-4811-bc16-febdf5347540-0')"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Extract the relevant information, if not explicitly provided do not guess. Extract partial info\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "extraction_chain = prompt | extraction_model"
      ],
      "metadata": {
        "id": "6gSKoqkqSM1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extraction_chain.invoke({\"input\": \"Joe is 30, his mom is Martha\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMEHIyfPSSJ3",
        "outputId": "e1f2af89-9b90-46b3-8f63-45ebe6d9b900"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"people\": [\\n    {\\n      \"name\": \"Joe\",\\n      \"age\": 30\\n    },\\n    {\\n      \"name\": \"Martha\",\\n      \"age\": null\\n    }\\n  ]\\n}', 'name': 'Information'}}, response_metadata={'token_usage': {'completion_tokens': 45, 'prompt_tokens': 93, 'total_tokens': 138}, 'model_name': 'gpt-4-32k', 'system_fingerprint': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {}}, id='run-85a05e06-0d42-4f93-9a1c-ebb5f961561e-0')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extraction_chain = prompt | extraction_model | JsonOutputFunctionsParser()\n",
        "\n",
        "extraction_chain.invoke({\"input\": \"Joe is 30, his mom is Martha\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvUsiX-sSYzt",
        "outputId": "4459e854-b222-43a7-faf9-3965b4ab030d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'people': [{'name': 'Joe', 'age': 30}, {'name': 'Martha', 'age': None}]}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser\n",
        "\n",
        "extraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"people\")\n",
        "\n",
        "extraction_chain.invoke({\"input\": \"Joe is 30, his mom is Martha\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btZl2M6ASfxR",
        "outputId": "19cf9e81-b607-4471-e98d-c63b6e0aee47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'Joe', 'age': 30}, {'name': 'Martha', 'age': None}]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extraction_chain.invoke({\"input\": \"\"\"Park Jimin, a sprightly 25-year-old with a passion for dance, greeted the morning at his bakery. Across the street, Dr. Amelia Hernandez, a seasoned physician at 62, reviewed patient charts. Young Maya Lopez, 8, skipped down the sidewalk, clutching a library book. Her grandfather, Mr. Chen, a retired carpenter with kind eyes (his age a delightful mystery), sat on a park bench, feeding pigeons.\n",
        "At the bustling cafe, Sarah Jones, a lawyer with a fierce intellect, sipped coffee with her friend, David Lee, a 30-year-old software developer. In the distance, a group of energetic teenagers, led by the ever-enthusiastic 17-year-old Gabriela Rodriguez, practiced for a school play.\n",
        "Ms. Evelyn Wright, a librarian with a wealth of knowledge (her age truly irrelevant), meticulously shelved books. Meanwhile, Mr. Tanaka, a 78-year-old with a youthful spirit, tended his immaculate flower garden.\n",
        "The city buzzed with life. Delivery person, Omar Khan, 22 and always reliable, wove through the streets. Ms. Garcia, a community leader with boundless energy (though some whispered her age might be closer to 80 than 70), organized a neighborhood cleanup.\n",
        "In the afternoon, young firefighter, Daniel Kim, 28 and brave, trained with his team. Nearby, Mrs. Patel, a tailor with a meticulous eye (age a mere number), meticulously crafted a beautiful sari.\n",
        "As the sun dipped below the horizon, young entrepreneur, Anika Sharma, 19 and full of ideas, brainstormed new business ventures. Across town, Mr. and Mrs. Thompson, a loving couple whose age didn't matter as they held hands, strolled through the park.\n",
        "This city, a tapestry woven from threads of countless lives, young and old, each with a story, a purpose, and a heart that beat to the rhythm of the bustling urban jungle.\"\"\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuyJ4y5MVR-x",
        "outputId": "c223d112-a590-4027-9f84-343cb9762ae8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'Park Jimin', 'age': 25},\n",
              " {'name': 'Dr. Amelia Hernandez', 'age': 62},\n",
              " {'name': 'Maya Lopez', 'age': 8},\n",
              " {'name': 'Mr. Chen', 'age': None},\n",
              " {'name': 'Sarah Jones', 'age': None},\n",
              " {'name': 'David Lee', 'age': 30},\n",
              " {'name': 'Gabriela Rodriguez', 'age': 17},\n",
              " {'name': 'Ms. Evelyn Wright', 'age': None},\n",
              " {'name': 'Mr. Tanaka', 'age': 78},\n",
              " {'name': 'Omar Khan', 'age': 22},\n",
              " {'name': 'Ms. Garcia', 'age': None},\n",
              " {'name': 'Daniel Kim', 'age': 28},\n",
              " {'name': 'Mrs. Patel', 'age': None},\n",
              " {'name': 'Anika Sharma', 'age': 19},\n",
              " {'name': 'Mr. Thompson', 'age': None},\n",
              " {'name': 'Mrs. Thompson', 'age': None}]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Doing it for real\n",
        "\n",
        "We can apply tagging to a larger body of text.\n",
        "\n",
        "For example, let's load this blog post and extract tag information from a sub-set of the text."
      ],
      "metadata": {
        "id": "93K9O-bdSl10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tagging\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
        "documents = loader.load()\n",
        "\n",
        "doc = documents[0]\n",
        "\n",
        "page_content = doc.page_content[:10000]\n",
        "\n",
        "\n",
        "\n",
        "class Overview(BaseModel):\n",
        "    \"\"\"Overview of a section of text.\"\"\"\n",
        "    summary: str = Field(description=\"Provide a concise summary of the content.\")\n",
        "    language: str = Field(description=\"Provide the language that the content is written in.\")\n",
        "    keywords: str = Field(description=\"Provide keywords related to the content.\")\n",
        "\n",
        "overview_tagging_function = [\n",
        "    convert_pydantic_to_openai_function(Overview)\n",
        "]\n",
        "tagging_model = model.bind(\n",
        "    functions=overview_tagging_function,\n",
        "    function_call={\"name\":\"Overview\"}\n",
        ")\n",
        "tagging_chain = prompt | tagging_model | JsonOutputFunctionsParser()\n",
        "\n",
        "tagging_chain.invoke({\"input\": page_content})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHsPBwPzS2ha",
        "outputId": "e04c363e-3ccf-49d7-e9f4-3776224875ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'summary': \"The blog post discusses the concept of building autonomous agents powered by Large Language Models (LLMs). It covers three main components of such a system: Planning, Memory, and Tool Use. In planning, the agent breaks down large tasks into smaller, manageable subgoals and performs self-reflection to improve. The memory component discusses the use of short-term and long-term memory in LLMs. The use of tools refers to the agent's ability to call external APIs for additional information. The post also discusses various case studies and challenges.\",\n",
              " 'language': 'English',\n",
              " 'keywords': 'Large Language Models, Autonomous Agents, Planning, Memory, Tool Use, Self-Reflection, Task Decomposition'}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Extraction\n",
        "class Paper(BaseModel):\n",
        "    \"\"\"Information about papers mentioned.\"\"\"\n",
        "    title: str\n",
        "    author: Optional[str]\n",
        "\n",
        "\n",
        "class Info(BaseModel):\n",
        "    \"\"\"Information to extract\"\"\"\n",
        "    papers: List[Paper]\n",
        "\n",
        "paper_extraction_function = [\n",
        "    convert_pydantic_to_openai_function(Info)\n",
        "]\n",
        "extraction_model = model.bind(\n",
        "    functions=paper_extraction_function,\n",
        "    function_call={\"name\":\"Info\"}\n",
        ")\n",
        "extraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"papers\")\n",
        "\n",
        "extraction_chain.invoke({\"input\": page_content})\n",
        "\n",
        "template = \"\"\"A article will be passed to you. Extract from it all papers that are mentioned by this article.\n",
        "\n",
        "Do not extract the name of the article itself. If no papers are mentioned that's fine - you don't need to extract any! Just return an empty list.\n",
        "\n",
        "Do not make up or guess ANY extra information. Only extract what exactly is in the text.\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", template),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "extraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"papers\")"
      ],
      "metadata": {
        "id": "rmSWiA3RTcF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extraction_chain.invoke({\"input\": page_content})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWEThwswTdhy",
        "outputId": "0e299893-3809-4cd0-8017-56361850a425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'title': 'Chain of thought (CoT)', 'author': 'Wei et al. 2022'},\n",
              " {'title': 'Tree of Thoughts', 'author': 'Yao et al. 2023'},\n",
              " {'title': 'LLM+P', 'author': 'Liu et al. 2023'},\n",
              " {'title': 'ReAct', 'author': 'Yao et al. 2023'},\n",
              " {'title': 'Reflexion', 'author': 'Shinn & Labash 2023'},\n",
              " {'title': 'Chain of Hindsight (CoH)', 'author': 'Liu et al. 2023'},\n",
              " {'title': 'Algorithm Distillation (AD)', 'author': 'Laskin et al. 2023'},\n",
              " {'title': 'RL^2', 'author': 'Duan et al. 2017'}]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "extraction_chain.invoke({\"input\": \"hi\"})"
      ],
      "metadata": {
        "id": "v9iXIJhQTkrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_overlap=0)\n",
        "\n",
        "splits = text_splitter.split_text(doc.page_content)\n",
        "\n",
        "len(splits)\n",
        "\n",
        "def flatten(matrix):\n",
        "    flat_list = []\n",
        "    for row in matrix:\n",
        "        flat_list += row\n",
        "    return flat_list\n",
        "\n",
        "flatten([[1, 2], [3, 4]])\n",
        "\n",
        "\n",
        "from langchain.schema.runnable import RunnableLambda\n",
        "\n",
        "prep = RunnableLambda(\n",
        "    lambda x: [{\"input\": doc} for doc in text_splitter.split_text(x)]\n",
        ")\n",
        "\n",
        "prep.invoke(\"hi\")\n",
        "\n",
        "chain = prep | extraction_chain.map() | flatten\n",
        "\n",
        "chain.invoke(doc.page_content)\n",
        "\n"
      ],
      "metadata": {
        "id": "5fqWrtJJK7tk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b3d0d1a-b0b1-427f-989f-39288c420ea7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'title': 'Chain of thought', 'author': 'Wei et al. 2022'},\n",
              " {'title': 'Tree of Thoughts', 'author': 'Yao et al. 2023'},\n",
              " {'title': 'LLM+P', 'author': 'Liu et al. 2023'},\n",
              " {'title': 'ReAct', 'author': 'Yao et al. 2023'},\n",
              " {'title': 'Reflexion', 'author': 'Shinn & Labash 2023'},\n",
              " {'title': 'Chain of Hindsight', 'author': 'Liu et al.'},\n",
              " {'title': 'Algorithm Distillation', 'author': 'Laskin et al.'},\n",
              " {'title': None, 'author': 'Laskin et al. 2023'},\n",
              " {'title': None, 'author': 'Duan et al. 2017'},\n",
              " {'title': None, 'author': 'Miller 1956'},\n",
              " {'title': 'MRKL', 'author': 'Karpas et al. 2022'},\n",
              " {'title': 'TALM', 'author': 'Parisi et al. 2022'},\n",
              " {'title': 'Toolformer', 'author': 'Schick et al. 2023'},\n",
              " {'title': 'HuggingGPT', 'author': 'Shen et al. 2023'},\n",
              " {'title': 'API-Bank', 'author': 'Li et al. 2023'},\n",
              " {'title': 'ChemCrow', 'author': 'Bran et al. 2023'},\n",
              " {'title': None, 'author': 'Boiko et al. (2023)'},\n",
              " {'title': 'Generative Agents', 'author': 'Park, et al. 2023'},\n",
              " {'title': 'Chain of thought prompting elicits reasoning in large language models.',\n",
              "  'author': 'Wei et al.'},\n",
              " {'title': 'Tree of Thoughts: Deliberate Problem Solving with Large Language Models.',\n",
              "  'author': 'Yao et al.'},\n",
              " {'title': 'Chain of Hindsight Aligns Language Models with Feedback',\n",
              "  'author': 'Liu et al.'},\n",
              " {'title': 'LLM+P: Empowering Large Language Models with Optimal Planning Proficiency',\n",
              "  'author': 'Liu et al.'},\n",
              " {'title': 'ReAct: Synergizing reasoning and acting in language models.',\n",
              "  'author': 'Yao et al.'},\n",
              " {'title': 'Announcing ScaNN: Efficient Vector Similarity Search',\n",
              "  'author': 'Google Blog'},\n",
              " {'title': 'Reflexion: an autonomous agent with dynamic memory and self-reflection',\n",
              "  'author': 'Shinn & Labash'},\n",
              " {'title': 'In-context Reinforcement Learning with Algorithm Distillation',\n",
              "  'author': 'Laskin et al.'},\n",
              " {'title': 'MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.',\n",
              "  'author': 'Karpas et al.'},\n",
              " {'title': 'Webgpt: Browser-assisted question-answering with human feedback.',\n",
              "  'author': 'Nakano et al.'},\n",
              " {'title': 'TALM: Tool Augmented Language Models', 'author': 'Parisi et al.'},\n",
              " {'title': 'Toolformer: Language Models Can Teach Themselves to Use Tools.',\n",
              "  'author': 'Schick et al.'},\n",
              " {'title': 'Why is Vector Search so fast?', 'author': 'Weaviate Blog'},\n",
              " {'title': 'API-Bank: A Benchmark for Tool-Augmented LLMs',\n",
              "  'author': 'Li et al.'},\n",
              " {'title': 'HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace',\n",
              "  'author': 'Shen et al.'},\n",
              " {'title': 'ChemCrow: Augmenting large-language models with chemistry tools.',\n",
              "  'author': 'Bran et al.'},\n",
              " {'title': 'Emergent autonomous scientific research capabilities of large language models.',\n",
              "  'author': 'Boiko et al.'},\n",
              " {'title': 'Generative Agents: Interactive Simulacra of Human Behavior.',\n",
              "  'author': 'Joon Sung Park, et al.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lesson 5 Tools and Routing"
      ],
      "metadata": {
        "id": "7T3MUsO8a3hz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import tool"
      ],
      "metadata": {
        "id": "eZoAHU9ka76c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def search(query:str)->str:\n",
        "  \"\"\"Search for weather online\"\"\"\n",
        "  return \"42f\""
      ],
      "metadata": {
        "id": "fFB1FRsZba0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search.name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "qZu3u5Kjboy8",
        "outputId": "05a89565-5650-4743-b030-14b886d5fbda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'search'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search.description"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "_xhwUcXKbrBT",
        "outputId": "3c24ac4b-1be5-42c1-c4d1-f860304d505e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'search(query: str) -> str - Search for weather online'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search.args"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m38TK2hYbt6p",
        "outputId": "766b2f27-9449-4e12-fd53-e22ca97e162f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': {'title': 'Query', 'type': 'string'}}"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic.v1 import BaseModel,Field\n",
        "class SearchInput(BaseModel):\n",
        "  query:str=Field(description=\"Thing to search for\")"
      ],
      "metadata": {
        "id": "0f0jvae0bwMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tool(args_schema=SearchInput)\n",
        "def search(query: str)->str:\n",
        "  \"\"\"Search for weather online\"\"\"\n",
        "  return \"42f\""
      ],
      "metadata": {
        "id": "Lz7E1C4icOV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search.args"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QP98VTHGdAhm",
        "outputId": "dc9c788c-2e2c-45ad-be5d-cdc032520914"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': {'title': 'Query', 'type': 'string'}}"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search.run('a')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "s6Lj_o6hdE-h",
        "outputId": "f63d4803-c2b7-4471-ae14-0752fa913d69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'42f'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pydantic.v1 import BaseModel, Field\n",
        "import datetime\n",
        "\n",
        "# Define the input schema\n",
        "class OpenMeteoInput(BaseModel):\n",
        "    latitude: float = Field(..., description=\"Latitude of the location to fetch weather data for\")\n",
        "    longitude: float = Field(..., description=\"Longitude of the location to fetch weather data for\")\n",
        "\n",
        "@tool(args_schema=OpenMeteoInput)\n",
        "def get_current_temperature(latitude: float, longitude: float) -> dict:\n",
        "    \"\"\"Fetch current temperature for given coordinates.\"\"\"\n",
        "\n",
        "    BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n",
        "\n",
        "    # Parameters for the request\n",
        "    params = {\n",
        "        'latitude': latitude,\n",
        "        'longitude': longitude,\n",
        "        'hourly': 'temperature_2m',\n",
        "        'forecast_days': 1,\n",
        "    }\n",
        "\n",
        "    # Make the request\n",
        "    response = requests.get(BASE_URL, params=params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        results = response.json()\n",
        "    else:\n",
        "        raise Exception(f\"API Request failed with status code: {response.status_code}\")\n",
        "\n",
        "    current_utc_time = datetime.datetime.utcnow()\n",
        "    time_list = [datetime.datetime.fromisoformat(time_str.replace('Z', '+00:00')) for time_str in results['hourly']['time']]\n",
        "    temperature_list = results['hourly']['temperature_2m']\n",
        "\n",
        "    closest_time_index = min(range(len(time_list)), key=lambda i: abs(time_list[i] - current_utc_time))\n",
        "    current_temperature = temperature_list[closest_time_index]\n",
        "\n",
        "    return f'The current temperature is {current_temperature}°C'"
      ],
      "metadata": {
        "id": "qDtYl9VIeZen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_current_temperature.run({\"latitude\":14,\"longitude\":13})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "XY-LIIN7f5RC",
        "outputId": "258a6354-b3e9-4814-a930-cf627e8852b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The current temperature is 45.6°C'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dstf_rFDnCQp",
        "outputId": "fd72ee81-5a46-4a21-ecf1-fd0d2351077c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "@tool\n",
        "def search_wikipedia(query: str) -> str:\n",
        "    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n",
        "    page_titles = wikipedia.search(query)\n",
        "    summaries = []\n",
        "    for page_title in page_titles[: 3]:\n",
        "        try:\n",
        "            wiki_page =  wikipedia.page(title=page_title, auto_suggest=False)\n",
        "            summaries.append(f\"Page: {page_title}\\nSummary: {wiki_page.summary}\")\n",
        "        except (\n",
        "            self.wiki_client.exceptions.PageError,\n",
        "            self.wiki_client.exceptions.DisambiguationError,\n",
        "        ):\n",
        "            pass\n",
        "    if not summaries:\n",
        "        return \"No good Wikipedia Search Result was found\"\n",
        "    return \"\\n\\n\".join(summaries)"
      ],
      "metadata": {
        "id": "DsU00EKJgu4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_wikipedia({\"query\":\"Wikipedia\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526
        },
        "id": "6JTWlHgSnXJW",
        "outputId": "754c5e66-364c-4359-eed9-23ba6c316c73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Page: Wikipedia\\nSummary: Wikipedia is a free content online encyclopedia written and maintained by a community of volunteers, known as Wikipedians, through open collaboration and the use of the wiki-based editing system MediaWiki. Wikipedia is the largest and most-read reference work in history. It is consistently ranked as one of the ten most popular websites in the world, and as of 2024 is ranked the fifth most visited website on the Internet by Semrush, and second by Ahrefs. Founded by Jimmy Wales and Larry Sanger on January 15, 2001, Wikipedia is hosted by the Wikimedia Foundation, an American nonprofit organization that employs a staff of over 700 people.\\nInitially only available in English, editions in other languages have been developed. Wikipedia's editions, when combined, comprise more than 62 million articles, attracting around 2 billion unique device visits per month and more than 14 million edits per month (about 5.2 edits per second on average)  as of November 2023. Roughly 26% of Wikipedia's traffic is from the United States, followed by Japan at 5.9%, the United Kingdom at 5.4%, Germany at 5%, Russia at 4.8%, and the remaining 54% split among other countries, according to data provided by Similarweb.\\nWikipedia has been praised for its enablement of the democratization of knowledge, extent of coverage, unique structure, and culture. It has been criticized for exhibiting systemic bias, particularly gender bias against women and geographical bias against the Global South (Eurocentrism). While the reliability of Wikipedia was frequently criticized in the 2000s, it has improved over time, receiving greater praise from the late 2010s onward while becoming an important fact-checking site.\\nWikipedia has been censored by some national governments, ranging from specific pages to the entire site. Articles on breaking news are often accessed as sources for frequently updated information about those events.\\n\\nPage: English Wikipedia\\nSummary: The English Wikipedia is the primary English-language edition of Wikipedia, an online encyclopedia. It was created by Jimmy Wales and Larry Sanger on 15 January 2001, as Wikipedia's first edition.\\nEnglish Wikipedia is hosted alongside other language editions by the Wikimedia Foundation, an American nonprofit organization. Its content is written independently of other editions in various varieties of English, aiming to stay consistent within articles. Its internal newspaper is The Signpost.\\nEnglish Wikipedia is the most-read version of Wikipedia, accounting for 48% of Wikipedia's cumulative traffic, with the remaining percentage split among the other languages. The English Wikipedia has the most articles of any edition, at 6,818,989 as of May 2024. It contains 10.8% of articles in all Wikipedias, although it lacks millions of articles found in other editions. The edition's one-billionth edit was made on 13 January 2021.\\nEnglish Wikipedia, often as a stand-in for Wikipedia overall, has been praised for its enablement of the democratization of knowledge, extent of coverage, unique structure, culture, and reduced degree of commercial bias. It has been criticized for exhibiting systemic bias, particularly gender bias against women and ideological bias. While its reliability was frequently criticized in the 2000s, it has improved over time, receiving greater praise in the late 2010s and early 2020s, having become an important fact-checking site. English Wikipedia has been characterized as having less cultural bias than other language editions due to its broader editor base.\\n\\n\\n\\nPage: Main Page\\nSummary: \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Routing"
      ],
      "metadata": {
        "id": "0hUilAwQoDNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools.render import format_tool_to_openai_function\n",
        "functions = [\n",
        "    format_tool_to_openai_function(f) for f in [\n",
        "        search_wikipedia, get_current_temperature\n",
        "    ]\n",
        "]\n",
        "model = model.bind(functions=functions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbeidmCNoFvN",
        "outputId": "c92815f2-fd9f-4cde-d142-51c54623c6f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The function `format_tool_to_openai_function` was deprecated in LangChain 0.1.16 and will be removed in 0.2.0. Use langchain_core.utils.function_calling.convert_to_openai_function() instead.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.invoke(\"What is current temperature of Chennai\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcFzgcemosrk",
        "outputId": "5172b453-1b5f-4cca-ec9e-b14e289a9587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"latitude\": 13.0827,\\n  \"longitude\": 80.2707\\n}', 'name': 'get_current_temperature'}}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 128, 'total_tokens': 157}, 'model_name': 'gpt-4-32k', 'system_fingerprint': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'function_call', 'logprobs': None, 'content_filter_results': {}}, id='run-71fecd89-d1e8-4a3b-ab79-b2f3b1cf13fc-0')"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are helpful but sassy assistant\"),\n",
        "    (\"user\", \"{input}\"),\n",
        "])\n",
        "chain = prompt | model"
      ],
      "metadata": {
        "id": "fGjMRxC0pi2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser"
      ],
      "metadata": {
        "id": "pViGZpqLpl_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | model | OpenAIFunctionsAgentOutputParser()"
      ],
      "metadata": {
        "id": "Ob6rDn4gpsHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = chain.invoke({\"input\": \"what is the weather in sf right now\"})"
      ],
      "metadata": {
        "id": "q9qV0ZflpseH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If openai does function call represented using AgentAction** \\\n",
        "**If openai doesn't function call represented using AgentFinish**"
      ],
      "metadata": {
        "id": "dlD_H8dIqxS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "yhnkD2-jpzPB",
        "outputId": "df0d7503-75d2-4bf0-8bac-ea5f4f776961"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain_core.agents.AgentActionMessageLog"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.agents.AgentActionMessageLog</b><br/>def __init__(tool: str, tool_input: Union[str, dict], log: str, **kwargs: Any)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/langchain_core/agents.py</a>A full description of an action for an ActionAgent to execute.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 84);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.tool"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "84pz4km4qNbI",
        "outputId": "76f80bcb-dc85-46ce-8b34-d2063d6bae0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'get_current_temperature'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.tool_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StP5mb8SqTyS",
        "outputId": "846d53a1-ecce-4b3a-95a7-15795d5469d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'latitude': 37.7749, 'longitude': -122.4194}"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.return_values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "0OurvZywqY2T",
        "outputId": "15649dcd-9712-42bb-d4a8-9384fd07933b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'AgentActionMessageLog' object has no attribute 'return_values'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-103-75f6c80a1e04>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'AgentActionMessageLog' object has no attribute 'return_values'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result2 = chain.invoke({\"input\": \"hi\"})"
      ],
      "metadata": {
        "id": "llm3Y9M8p4jW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(result2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "lxoCKRo2qIrU",
        "outputId": "eb8f3e17-c773-451f-9bb6-273e83f1b701"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain_core.agents.AgentFinish"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.agents.AgentFinish</b><br/>def __init__(return_values: dict, log: str, **kwargs: Any)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/langchain_core/agents.py</a>The final return value of an ActionAgent.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 113);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result2.return_values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fl3fGr7fqhhf",
        "outputId": "d26ebd49-0ba4-4665-9d69-88f4df8f6e15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'output': 'Hello there! How can I assist you today, preferably with a dash of sass?'}"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"input\":\"Where is SF located? and what is current temp in SF?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyFW2QR0waJB",
        "outputId": "db065f64-8801-4660-fca9-8916cf7e1e09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AgentActionMessageLog(tool='search_wikipedia', tool_input={'query': 'San Francisco'}, log=\"\\nInvoking: `search_wikipedia` with `{'query': 'San Francisco'}`\\n\\n\\n\", message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"query\": \"San Francisco\"\\n}', 'name': 'search_wikipedia'}}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 143, 'total_tokens': 160}, 'model_name': 'gpt-4-32k', 'system_fingerprint': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'function_call', 'logprobs': None, 'content_filter_results': {}}, id='run-3edf00a8-d03c-4cf4-a40f-929e328b8ad7-0')])"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.agent import AgentFinish\n",
        "def route(result):\n",
        "    if isinstance(result, AgentFinish):\n",
        "        return result.return_values['output']\n",
        "    else:\n",
        "        tools = {\n",
        "            \"search_wikipedia\": search_wikipedia,\n",
        "            \"get_current_temperature\": get_current_temperature,\n",
        "        }\n",
        "        return tools[result.tool].run(result.tool_input)"
      ],
      "metadata": {
        "id": "ygGoMfeyqnmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | model | OpenAIFunctionsAgentOutputParser() | route"
      ],
      "metadata": {
        "id": "V9rUnOFwrLri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"input\": \"What is the weather in san francisco right now?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QV90Mf-jrRq9",
        "outputId": "45454f21-b605-48ef-eb52-7cf35ce6ca62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The current temperature is 11.9°C'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"input\": \"hi\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "zpw4p8I0rWqD",
        "outputId": "9495e821-6af6-445b-fa80-60d00c1ec43b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello there! How can I assist you today, with a sprinkle of sass?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"input\":\"Where is SF located? and what is current temp in SF?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "KvzJhP64wJvL",
        "outputId": "d6688c67-3e0b-4080-aa52-036fed890b35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"San Francisco, often abbreviated as SF, is located in Northern California, USA. It's on the tip of a peninsula surrounded by the Pacific Ocean and San Francisco Bay. It's known for its hilly landscape, iconic Golden Gate Bridge, and year-round fog, among many other things.\\n\\nAs for the current temperature in San Francisco, unfortunately, I don't have real-time capabilities or access to live data feeds. To get the current temperature, you might want to check a reliable weather service or use a smart device that can provide you with real-time weather updates. Now, don't go out dressed for the wrong weather and blame it on me! Stay savvy and check that forecast.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing parallel function call"
      ],
      "metadata": {
        "id": "Y5VkJ92bxG1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools.render import format_tool_to_openai_function\n",
        "functions = [\n",
        "    format_tool_to_openai_function(f) for f in [\n",
        "        search_wikipedia, get_current_temperature\n",
        "    ]]\n",
        "model_with_parallel=model.bind_tools(tools=functions)"
      ],
      "metadata": {
        "id": "pfbuhioWxLPy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fee0579-0dc8-413f-cdf2-60ea3aec6cda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The function `format_tool_to_openai_function` was deprecated in LangChain 0.1.16 and will be removed in 0.2.0. Use langchain_core.utils.function_calling.convert_to_openai_function() instead.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_parallel.invoke({\"input\":\"Where is SF located? and what is current temp in SF?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "X16nq2HvxgIw",
        "outputId": "69bd9893-1ec3-4ee4-aca8-88d8b15213ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-6894f3cd7995>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_with_parallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"Where is SF located? and what is current temp in SF?\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4523\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4524\u001b[0m     ) -> Output:\n\u001b[0;32m-> 4525\u001b[0;31m         return self.bound.invoke(\n\u001b[0m\u001b[1;32m   4526\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4527\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0mChatGeneration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             self.generate_prompt(\n\u001b[0;32m--> 159\u001b[0;31m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"callbacks\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_convert_input\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mChatPromptValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    143\u001b[0m                 \u001b[0;34mf\"Invalid input type {type(input)}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0;34m\"Must be a PromptValue, str, or list of BaseMessages.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are helpful but sassy assistant. you have access to following tools get_current_temperature and search_wikipedia. \\\n",
        "    You can use both tools in parallel to answer user query.\\\n",
        "    if user question has two sub questions you both tools to answer the query \"),\n",
        "    (\"user\", \"{input}\"),\n",
        "])\n",
        "chain_with_tools = prompt | model_with_parallel"
      ],
      "metadata": {
        "id": "rcmgSeS2xujg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain_with_tools.invoke({\"input\":\"what is current temp in SF? and explain about SF\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkSBD5yqx30P",
        "outputId": "d54244ef-d302-48be-ccd5-806112ca1b81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_kiDRmgczpC4rt0CtNB57Sgde', 'function': {'arguments': '{\"latitude\": 37.7749, \"longitude\": -122.4194}', 'name': 'get_current_temperature'}, 'type': 'function'}, {'id': 'call_PACArOKPi6LejtfchPuJOHNV', 'function': {'arguments': '{\"query\": \"San Francisco\"}', 'name': 'search_wikipedia'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 93, 'prompt_tokens': 183, 'total_tokens': 276}, 'model_name': 'gpt-4', 'system_fingerprint': 'fp_2f57f81c11', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'tool_calls', 'logprobs': None, 'content_filter_results': {}}, id='run-9a4fefcf-4e83-4efd-8e40-e4bd25b6afa9-0', tool_calls=[{'name': 'get_current_temperature', 'args': {'latitude': 37.7749, 'longitude': -122.4194}, 'id': 'call_kiDRmgczpC4rt0CtNB57Sgde'}, {'name': 'search_wikipedia', 'args': {'query': 'San Francisco'}, 'id': 'call_PACArOKPi6LejtfchPuJOHNV'}])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://python.langchain.com/docs/use_cases/tool_use/parallel/"
      ],
      "metadata": {
        "id": "4-_qCYucy9xA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser"
      ],
      "metadata": {
        "id": "8o2Fm6hQzS2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "\n",
        "@tool\n",
        "def multiply(first_int: int, second_int: int) -> int:\n",
        "    \"\"\"Multiply two integers together.\"\"\"\n",
        "    return first_int * second_int\n",
        "\n",
        "\n",
        "@tool\n",
        "def add(first_int: int, second_int: int) -> int:\n",
        "    \"Add two integers.\"\n",
        "    return first_int + second_int\n",
        "\n",
        "\n",
        "@tool\n",
        "def exponentiate(base: int, exponent: int) -> int:\n",
        "    \"Exponentiate the base to the exponent power.\"\n",
        "    return base**exponent"
      ],
      "metadata": {
        "id": "qF-BAdfiy_hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from typing import Dict, List, Union\n",
        "\n",
        "from langchain_core.messages import AIMessage\n",
        "from langchain_core.runnables import (\n",
        "    Runnable,\n",
        "    RunnableLambda,\n",
        "    RunnableMap,\n",
        "    RunnablePassthrough,\n",
        ")\n",
        "\n",
        "tools = [multiply, exponentiate, add]\n",
        "llm_with_tools = model.bind_tools(tools)\n",
        "tool_map = {tool.name: tool for tool in tools}\n",
        "\n",
        "\n",
        "def call_tools(msg: AIMessage) -> Runnable:\n",
        "    \"\"\"Simple sequential tool calling helper.\"\"\"\n",
        "    tool_map = {tool.name: tool for tool in tools}\n",
        "    tool_calls = msg.tool_calls.copy()\n",
        "    for tool_call in tool_calls:\n",
        "        tool_call[\"output\"] = tool_map[tool_call[\"name\"]].invoke(tool_call[\"args\"])\n",
        "    return tool_calls\n",
        "\n",
        "\n",
        "chain = llm_with_tools | call_tools"
      ],
      "metadata": {
        "id": "XV_-u2c7zjXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_with_tools.invoke(\"What's 23 times 7, and what's five times 18 and add a million plus a billion and cube thirty-seven\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwzbBLTGzxS9",
        "outputId": "1758126a-6108-4741-9730-9648d8edf486"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_g81ORPtQOCfjgsBmHSrWn83v', 'function': {'arguments': '{\"first_int\": 23, \"second_int\": 7}', 'name': 'multiply'}, 'type': 'function'}, {'id': 'call_Qd5hZ3Z6Qx2Kp4kc8EVPjlMf', 'function': {'arguments': '{\"first_int\": 5, \"second_int\": 18}', 'name': 'multiply'}, 'type': 'function'}, {'id': 'call_1ZbbiIoeCxGvRJNMrPANfwk1', 'function': {'arguments': '{\"first_int\": 1000000, \"second_int\": 1000000000}', 'name': 'add'}, 'type': 'function'}, {'id': 'call_kUv4eic0QmxGoqiN7TMCGebY', 'function': {'arguments': '{\"base\": 37, \"exponent\": 3}', 'name': 'exponentiate'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 171, 'prompt_tokens': 168, 'total_tokens': 339}, 'model_name': 'gpt-4', 'system_fingerprint': 'fp_2f57f81c11', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'tool_calls', 'logprobs': None, 'content_filter_results': {}}, id='run-8e6914fb-f0e1-4c60-817f-3f7d5aa1c853-0', tool_calls=[{'name': 'multiply', 'args': {'first_int': 23, 'second_int': 7}, 'id': 'call_g81ORPtQOCfjgsBmHSrWn83v'}, {'name': 'multiply', 'args': {'first_int': 5, 'second_int': 18}, 'id': 'call_Qd5hZ3Z6Qx2Kp4kc8EVPjlMf'}, {'name': 'add', 'args': {'first_int': 1000000, 'second_int': 1000000000}, 'id': 'call_1ZbbiIoeCxGvRJNMrPANfwk1'}, {'name': 'exponentiate', 'args': {'base': 37, 'exponent': 3}, 'id': 'call_kUv4eic0QmxGoqiN7TMCGebY'}])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IcoR7dWoztjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lesson 6 Conversational Agents"
      ],
      "metadata": {
        "id": "0PrZdONrUrix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agents Basics\n",
        "\n",
        "* Agents\n",
        "  * are combination of LLM and code\n",
        "  * LLM reason about what steps to take and call for actions\n",
        "\n",
        "* Agent Loop\n",
        "  * Choose a tool to use\n",
        "  * Observe the output of tool\n",
        "  * Repeat until stop condition is met\n",
        "* Stopping conditions can be:\n",
        " * LLM determined\n",
        " *Hardcoded rules like max iterations\n",
        "\n"
      ],
      "metadata": {
        "id": "Byk87aBBU5EV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import tool"
      ],
      "metadata": {
        "id": "oSuQvaU_UhUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pydantic.v1 import BaseModel, Field\n",
        "import datetime\n",
        "\n",
        "# Define the input schema\n",
        "class OpenMeteoInput(BaseModel):\n",
        "    latitude: float = Field(..., description=\"Latitude of the location to fetch weather data for\")\n",
        "    longitude: float = Field(..., description=\"Longitude of the location to fetch weather data for\")\n",
        "\n",
        "@tool(args_schema=OpenMeteoInput)\n",
        "def get_current_temperature(latitude: float, longitude: float) -> dict:\n",
        "    \"\"\"Fetch current temperature for given coordinates.\"\"\"\n",
        "\n",
        "    BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n",
        "\n",
        "    # Parameters for the request\n",
        "    params = {\n",
        "        'latitude': latitude,\n",
        "        'longitude': longitude,\n",
        "        'hourly': 'temperature_2m',\n",
        "        'forecast_days': 1,\n",
        "    }\n",
        "\n",
        "    # Make the request\n",
        "    response = requests.get(BASE_URL, params=params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        results = response.json()\n",
        "    else:\n",
        "        raise Exception(f\"API Request failed with status code: {response.status_code}\")\n",
        "\n",
        "    current_utc_time = datetime.datetime.utcnow()\n",
        "    time_list = [datetime.datetime.fromisoformat(time_str.replace('Z', '+00:00')) for time_str in results['hourly']['time']]\n",
        "    temperature_list = results['hourly']['temperature_2m']\n",
        "\n",
        "    closest_time_index = min(range(len(time_list)), key=lambda i: abs(time_list[i] - current_utc_time))\n",
        "    current_temperature = temperature_list[closest_time_index]\n",
        "\n",
        "    return f'The current temperature is {current_temperature}°C'"
      ],
      "metadata": {
        "id": "RU2gTE_HUvNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "@tool\n",
        "def search_wikipedia(query: str) -> str:\n",
        "    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n",
        "    page_titles = wikipedia.search(query)\n",
        "    summaries = []\n",
        "    for page_title in page_titles[: 3]:\n",
        "        try:\n",
        "            wiki_page =  wikipedia.page(title=page_title, auto_suggest=False)\n",
        "            summaries.append(f\"Page: {page_title}\\nSummary: {wiki_page.summary}\")\n",
        "        except (\n",
        "            self.wiki_client.exceptions.PageError,\n",
        "            self.wiki_client.exceptions.DisambiguationError,\n",
        "        ):\n",
        "            pass\n",
        "    if not summaries:\n",
        "        return \"No good Wikipedia Search Result was found\"\n",
        "    return \"\\n\\n\".join(summaries)"
      ],
      "metadata": {
        "id": "jaeVQTpJUWVc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}